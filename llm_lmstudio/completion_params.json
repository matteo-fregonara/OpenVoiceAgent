{
    "model": "llama-3.2-3b-instruct",
    "parameters": {
        "temperature": 0.7,
        "max_tokens": 1000,
        "top_p": 1,
        "frequency_penalty": 0,
        "presence_penalty": 0,
        "stop": ["\n\nHuman:", "\n\nAssistant:"]
    }
}   